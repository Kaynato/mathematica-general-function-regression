005_data.txt


expsin:
	Fitfunction = Sin[30*#1*#2 + #4] Exp[-#1*#3] &;

	005_expsin_001.pdf:
		300 Length, 300 Sampling Rate
		InitFunctionData[5., {}, 0.2, NormalDistribution[1, 3.], {}, {}];
		InitIterationData[0.01, {RecoverDataFreq[2]/30, 1, 0}];
		Smoothness = 4;

	005_expsin_002.pdf:
		300 Length, 300 Sampling Rate
		InitFunctionData[5., {}, 0.2, NormalDistribution[1, 3.], {}, {}];
		InitIterationData[0.01, {RecoverDataFreq[1]/30, 1, 0}];
		Smoothness = 5; (best)
		Zigzagging observed on steepest part - bad behavior.

	{-1.8843439074598045`, 1.7127700566735733`, -2.92633344958303`}



	Start @ {RecoverDataFreq[2]/30, 1, -2.9}
	Very strange. NLCJG.

		Re-implement kickback to ensure that error decreases!
		Or just do the backtracking line search?


	Params:
		{4.72936, -3.42979, -1.40576}
		The problem is that conj.grad doesn't ensure steepest descent.
		
		This is bad with periodic functions...
		But extremely fast with polynomial and pure exponential.
			Exppoly
			Fitfunction = Exp[-#1*#3] + #1^2*#2 + #4 &;
			(Within 5!! WITHIN 5 ITERATIONS.)

			Exppoly2
			Good

		CJGRAD TROUBLES
			Sinusoidal (explosive)
			Anything without a constant term (presumably it affords a degree of freedom)

		Good with
			Arbitrary Degree Polynomials with a constant term
			Exponentials (normalization is a possible cure)

			normalization does not help to cure
			Example of 
			{2.338137629912179`, -1.68314106518711`, -2.241591949305569`}
			Fitfunction = #3 Exp[#1*#2] + #4 &;

		Extremely slow.
			Does not seem to produce optimal results for some exponential functions if the starting location is not very close.

		...Fixed: Adaptive tolerance on GRS.
			Some problems arose due to bugs but they were fixed soon enough.

			Mathematica is dying on me.

			After some adjustments to the GRS, this was improved as seen in 007_exp_acjg_001.pdf
				Function really doesn't want to reverse concavity.

				007_exp_acjg_002.pdf - resistance to concavity reversal
				
				But - in fact, this is a matter also of the normal negative gradient function.
				See 007_exp_ngrad_002.pdf

					There is a "wall" where the concavity of the exponential function is zero. Beware!


		Concavity flipping test:
			Fitfunction = Exp[#1*#2] + #3 &;

		Divergence:
			At high #2 it also has the risk of divergence.
			Using the initial parameters {x, 7.} and varying x from 4 to 5 we can see where it begins to diverge on the parameter space.
				See 007_exp_acjg_003.pdf for an example of overshoot		7.0
				See 007_exp_acjg_004.pdf for an example of convergence		6.2

		Some sinusoids:
			007_sin_acjg_001.pdf
			It is still very bad for sinusoids.
				But that is what Fourier transforms are for - that is not the use case intended for this program.

			007_sinexp_acjg_002.pdf - low freq decay
			007_sinexp_acjg_003.pdf - low freq decay
			007_sinexp_acjg_004.pdf - high freq decay
				Good examples.
			007_sinexp_acjg_005.pdf - high freq decay
			007_sinexp_acjg_006.pdf - medium freq decay
				Not so good, but overall it fits. The algorithm quickly stalls at a suboptimal distance.
				Despite its relative speed in terms of iterations, it is very computationally expensive.
				Compare:
			007_sinexp_gag_006.pdf
				General adaptive gradient developed earlier.
				More robust - despite taking many more iterations, it ACHIEVES MORE IN LESS ACTUAL TIME.
				Almost instantaneous compared to the acjg, and MUCH more reliable.

		008_concavity_gag_001.pdf
		008_concavity_acjg_001.pdf
			Concavity flipping is a problem for everyone.
		008_concavity_acjg_002.pdf
			Under the right conditions, however, this has a decent span of convergence.
			1.53298 seconds for 30 iterations.
		008_concavity_gag_001.pdf
			0.887821 seconds for 120 iterations.


	Overall, the old method seems to actually perform more effectively.


	The angular comparison method DOES NOT WORK.

	I made a "pure conjugate gradient" without any additional modifiers.
		It's faster and more effective.
		pcjg

	The report begins here.

		All generated with parameters:
			InitFunctionData[5., {}, 0.2, NormalDistribution[1, 0.9], {}, {}];

		010_polyn_pcjg_...
		Polynomials (5, degrees 100, 100, 50, 20, 5):
			Generated with random parameters.
			Initialized position with random parameters.
			Extremely good performance.

		Exponentials.
			#4 Exp[#1 #2] + #3
			Not normalized
				010_exp_pcjg_...
					Take care to start on the right concavity.
					005 particularly is notable for making it through the concavity issue (though slowly) as the error plot shows also.

		Those with 50 data points were in 011.
		Combinations of cosine and sine is poor.
		Sums of sinusoidal and exponential were poor even with extraction of frequency to aid the initialization.

		Noise:

			